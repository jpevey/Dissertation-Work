{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "%matplotlib inline\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "#from keras_radam import RAdam\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import time\n",
    "import sys\n",
    "import collections\n",
    "# Keras turner imports\n",
    "import kerastuner\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization \n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_directory = \"Z:\\\\FNS_GA_v5_100_gen_CNN_hyperparam_work\\\\models\\\\total_flux\\\\models\"\n",
    "data_directory = \"Z:\\\\FNS_GA_v5_100_gen_CNN_hyperparam_work\\\\models\\\\total_flux\\\\data\"\n",
    "train_data_string = [\"X_tf_times_source_training_data.csv.npy\",\n",
    "                     \"Y_tf_times_source_training_data.csv.npy\"]\n",
    "validation_data_string = [\"X_tf_times_source_validation_data.csv.npy\",\n",
    "                          \"Y_tf_times_source_validation_data.csv.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ = np.loadtxt(\"FNS_GA_Data_Runs_11_12_Paper_2.csv\", delimiter=\",\")\n",
    "#X = data_[:,0:150]\n",
    "os.chdir(data_directory)\n",
    "X_train = np.load(train_data_string[0])\n",
    "Y_train = np.load(train_data_string[1])\n",
    "\n",
    "\n",
    "#data_test = np.loadtxt(\"FNS_GA_Run_13_Data_Test.csv\", delimiter=\",\")\n",
    "X_test = np.load(validation_data_string[0])\n",
    "Y_test = np.load(validation_data_string[1])\n",
    "\n",
    "#X_test = np.load(\"test_2x2_fns_run_test_dat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "class HyperModel_555_v2(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "    def build(self, hp):\n",
    "        conv_size = hp.Int(\"first_conv_size\", min_value=16, max_value=64, step=1)\n",
    "        kernel_size = hp.Int(\"kernel_size\", min_value=4, max_value=10, step=1)\n",
    "        dense_layer_size = hp.Int(\"dense_layer_size\", min_value=32, max_value=128, step=32)\n",
    "        max_pool_val = hp.Int(\"max_pool_val\", min_value=2, max_value=10, step=1)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(conv_size, kernel_size=(kernel_size, 2, 2),\n",
    "                         input_shape=self.input_shape, padding='same', data_format=\"channels_last\", strides = (1, 1, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(BatchNormalization())\n",
    "        ##model.add(Conv3D(32, (3, 3, 5), activation=LeakyReLU(alpha=0.3), padding='same'))\n",
    "\n",
    "        for i in range(hp.Int(\"n_layers\", 7, 12)):\n",
    "            model.add(Conv3D(hp.Int(f\"conv_{i}_size\",\n",
    "                                    min_value=16,\n",
    "                                    max_value=64,\n",
    "                                    step=8),\n",
    "                             (kernel_size, 2, 2),\n",
    "                             padding='same'))\n",
    "            model.add(LeakyReLU(alpha=0.3))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(max_pool_val, 2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(dense_layer_size))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(dense_layer_size))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(hp.Float('dropout_value', min_value = 0.0, max_value = 0.4, step=0.1)))\n",
    "        \n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        ### Building the optimizer object\n",
    "        #sgd = optimizers.SGD(lr = 0.1) # 1.0 0.0001 0.01\n",
    "        opt = optimizers.Adam(lr = hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=0.0001,\n",
    "                    max_value=0.04,\n",
    "                    default=0.0001,\n",
    "                    step=1e-4)) \n",
    "           \n",
    "        ### Adding the loss function and optimizer to the model\n",
    "        #mcp_save = ModelCheckpoint('CNN_3d_2x2_test_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "        #opt = RAdam(total_steps=100, warmup_proportion=0.1, min_lr=1e-5)\n",
    "        model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "        ### Running the model\n",
    "\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (60, 2, 2, 4)\n",
    "hypermodel = HyperModel_555_v2(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(run_directory)\n",
    "LOG_DIR = \"bayes_tf_times_source_1_\"\n",
    "max_trials_val = 100\n",
    "# int(max_trials_val/10)\n",
    "tuner_bo = BayesianOptimization(\n",
    "            hypermodel,\n",
    "            objective='val_mae',\n",
    "            max_trials=max_trials_val,\n",
    "            seed=1,\n",
    "            executions_per_trial=2,\n",
    "            num_initial_points = int(max_trials_val/10),\n",
    "            directory = LOG_DIR)\n",
    "##tuner_bo.search(x=X_train, y=Y_train,\n",
    "##             epochs=300,\n",
    "##             batch_size=2000,\n",
    "##             validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tuner_bo.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#best_model = tuner_bo.get_best_models(num_models=10)\n",
    "#best_model[0].save('best_model_bayes_test_keff_gen_46_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_models(models_list,\n",
    "                     model_string_template = \"test\",\n",
    "                     csv_logger_bool = True,\n",
    "                     model_checkpoint_boo = True,\n",
    "                     save_original_model = True,\n",
    "                     train_models = True,\n",
    "                     train_model_options = {}):\n",
    "    \n",
    "    for model_count, current_model in enumerate(models_list):\n",
    "        model_string = model_string_template + \"_model_\" + str(model_count)\n",
    "        \n",
    "        callback_list = []\n",
    "        if csv_logger_bool:\n",
    "            csv_logger = CSVLogger(model_string + '.log')\n",
    "            callback_list.append(csv_logger)\n",
    "            \n",
    "        if model_checkpoint_boo:\n",
    "            checkpoint_filepath = model_string + \"_chkpnt_.{epoch:02d}-{val_mae:.6f}.hdf5\"\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                            filepath=checkpoint_filepath,\n",
    "                                            monitor='val_mae',\n",
    "                                            mode='min',\n",
    "                                            save_best_only=True,\n",
    "                                            verbose = 1)\n",
    "            callback_list.append(model_checkpoint_callback)\n",
    "            \n",
    "        if save_original_model:\n",
    "            current_model.save(model_string + '_original')\n",
    "            \n",
    "        if train_models:\n",
    "            current_model.fit(train_model_options['x_train'],\n",
    "                 train_model_options['y_train'],\n",
    "                 validation_data=train_model_options['validation_data'],\n",
    "                 epochs=train_model_options['epochs'],\n",
    "                 batch_size=train_model_options['batch_size'],\n",
    "                 verbose=train_model_options['verbose'],\n",
    "                 callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner_bo.get_best_models(num_models=10)\n",
    "train_model_opt = collections.OrderedDict()\n",
    "train_model_opt['x_train'] = X_train\n",
    "train_model_opt['y_train'] = Y_train\n",
    "train_model_opt['validation_data'] = (X_test, Y_test)\n",
    "train_model_opt['epochs'] = 1000\n",
    "train_model_opt['batch_size'] = int(len(X_train)/2)\n",
    "train_model_opt['verbose'] = 1\n",
    "\n",
    "\n",
    "train_top_models(best_models, train_model_options = train_model_opt,model_string_template = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"tf_model_1_chkpnt_.1737-1925532.000000.hdf5\")\n",
    "print(loaded_model.summary())\n",
    "output_ = loaded_model.predict(X_test)\n",
    "for real, pred in zip(Y_test, output_):\n",
    "    print(real[0], pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model_opt = collections.OrderedDict()\n",
    "train_model_opt['x_train'] = X_train\n",
    "train_model_opt['y_train'] = Y_train\n",
    "train_model_opt['validation_data'] = (X_test, Y_test)\n",
    "train_model_opt['epochs'] = 3000\n",
    "train_model_opt['batch_size'] = int(len(X_train)/4)\n",
    "train_model_opt['verbose'] = 1\n",
    "\n",
    "\n",
    "train_top_models([loaded_model], model_string_template = \"model_6_tf\", train_model_options = train_model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"model_6_tf_model_0_chkpnt_.707-0.000382.hdf5\")\n",
    "print(loaded_model.summary())\n",
    "output_ = loaded_model.predict(X_test)\n",
    "for real, pred in zip(Y_test, output_):\n",
    "    print(real[0], pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('*.hdf5') # * means all if need specific format then *.csv\n",
    "real_list = []\n",
    "for file in list_of_files:\n",
    "    if \"model_7\" in file:\n",
    "        real_list.append(file)\n",
    "latest_file = max(real_list, key=os.path.getctime)\n",
    "print(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_files(file_flag, X_data, Y_data, file_count_value = 10, file_suffix = '.hdf5'):\n",
    "    \n",
    "    list_of_files = glob.glob('*.hdf5') # * means all if need specific format then *.csv\n",
    "    real_list = []\n",
    "    list_of_best_models = []\n",
    "        \n",
    "    for file_count in range(file_count_value):\n",
    "        file_flag_ = file_flag + str(file_count)\n",
    "        #print(file_flag_)\n",
    "        for file in list_of_files:\n",
    "            if file_flag_ in file:\n",
    "                real_list.append(file)\n",
    "        latest_file = max(real_list, key=os.path.getctime)    \n",
    "        #print(latest_file)\n",
    "        list_of_best_models.append(latest_file)\n",
    "        \n",
    "    pred_data = []\n",
    "    for model in list_of_best_models:\n",
    "        loaded_model = keras.models.load_model(model)\n",
    "        pred_data.append(loaded_model.predict(X_data))\n",
    "        \n",
    "    for count, _ in enumerate(pred_data[0]):\n",
    "        print_str = ''\n",
    "        for count_model, __ in enumerate(pred_data):\n",
    "            #print(pred_data[count_model][0][0])\n",
    "            print_str += str(pred_data[count_model][count][0]) + \",\" \n",
    "        print(str(Y_data[count][0]) + \",\" + print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latest_files('tf_model_', X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"model_6_model_0_chkpnt_.338-0.007420.hdf5\")\n",
    "output_ =loaded_model.predict(X_train)\n",
    "for real, pred in zip(Y_train, output_):\n",
    "    print(real[0], pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.5,\n",
    "                              patience=150, min_lr=0.00001, verbose = 1)\n",
    "csv_logger = CSVLogger('keff_long_training.log')\n",
    "\n",
    "checkpoint_filepath = ''\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.fit(X_train,\n",
    "                 Y_train,\n",
    "                 validation_data=(X_test, Y_test),\n",
    "                 epochs=3000,\n",
    "                 batch_size=1000,\n",
    "                 verbose=1,\n",
    "                 callbacks=[reduce_lr, csv_logger, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"bayes_test_rep_gen_46_4_100_epoch_\"\n",
    "max_trials_val = 100\n",
    "# int(max_trials_val/10)\n",
    "tuner_bo = BayesianOptimization(\n",
    "            hypermodel,\n",
    "            objective='val_mae',\n",
    "            max_trials=max_trials_val,\n",
    "            seed=1,\n",
    "            executions_per_trial=2,\n",
    "            num_initial_points = int(max_trials_val/10),\n",
    "            directory = LOG_DIR)\n",
    "tuner_bo.search(x=X_train, y=Y_train,\n",
    "             epochs=100,\n",
    "             batch_size=2000,\n",
    "             validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[0].save('best_model_2_1_3ed_fns_5_mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"best_model_2_1_3ed_fns_5_mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=1000, batch_size=2500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = open(\"out_2.csv\", \"w\")\n",
    "for _ in test_pred:\n",
    "    str_ =str(_[0])+\"\\n\"\n",
    "    outputfile.write(str_)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
