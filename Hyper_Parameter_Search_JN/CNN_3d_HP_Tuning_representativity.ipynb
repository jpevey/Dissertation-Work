{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_directory = \"Z:\\\\FNS_GA_v5_100_gen_CNN_hyperparam_work\\\\models\\\\representativity\\\\models\"\n",
    "data_directory = \"Z:\\\\FNS_GA_v5_100_gen_CNN_hyperparam_work\\\\models\\\\representativity\\\\data\"\n",
    "train_data_string = [\"X_rep_training_data.csv.npy\",\n",
    "                     \"Y_rep_training_data.csv.npy\"]\n",
    "validation_data_string = [\"X_rep_validation_data.csv.npy\",\n",
    "                          \"Y_rep_validation_data.csv.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.1.0\n",
      "Keras Version: 2.2.4-tf\n",
      "\n",
      "Python 3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.2.1\n",
      "Scikit-Learn 0.24.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "%matplotlib inline\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "#from keras_radam import RAdam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization \n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ = np.loadtxt(\"FNS_GA_Data_Runs_11_12_Paper_2.csv\", delimiter=\",\")\n",
    "#X = data_[:,0:150]\n",
    "os.chdir(data_directory)\n",
    "X_train = np.load(train_data_string[0])\n",
    "Y_train = np.load(train_data_string[1])\n",
    "\n",
    "\n",
    "#data_test = np.loadtxt(\"FNS_GA_Run_13_Data_Test.csv\", delimiter=\",\")\n",
    "X_test = np.load(validation_data_string[0])\n",
    "Y_test = np.load(validation_data_string[1])\n",
    "\n",
    "#X_test = np.load(\"test_2x2_fns_run_test_dat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_model():\n",
    "#    model = Sequential()\n",
    "#    model.add(Conv3D(24, kernel_size=(2, 2, 2),\n",
    "#                     input_shape=(60, 2, 2, 4), padding='same', data_format=\"channels_last\", strides = (1, 1, 1)))\n",
    "#    model.add(LeakyReLU(alpha=0.3))\n",
    "#    model.add(BatchNormalization())\n",
    "#    ##model.add(Conv3D(32, (3, 3, 5), activation=LeakyReLU(alpha=0.3), padding='same'))\n",
    "#    for _ in range(5):\n",
    "#        model.add(Conv3D(24, (5, 2, 2), padding='same'))\n",
    "#        model.add(LeakyReLU(alpha=0.3))\n",
    "#        model.add(BatchNormalization())\n",
    "#    model.add(MaxPooling3D(pool_size=(6, 2, 2)))\n",
    "#    model.add(Flatten())\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Dense(128))\n",
    "#    model.add(LeakyReLU(alpha=0.3))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Dense(128))\n",
    "#    model.add(LeakyReLU(alpha=0.3))\n",
    "#    model.add(Dropout(0.25))\n",
    "#    model.add(Dense(1, activation='linear'))\n",
    "#\n",
    "#    ### Building the optimizer object\n",
    "#    #sgd = optimizers.SGD(lr = 0.1) # 1.0 0.0001 0.01\n",
    "#    opt = optimizers.Adam(lr = 0.005) # 1.0 0.0001 0.01\n",
    "#    ### Adding the loss function and optimizer to the model\n",
    "#    mcp_save = ModelCheckpoint('CNN_3d_2x2_test_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "#    #opt = RAdam(total_steps=100, warmup_proportion=0.1, min_lr=1e-5)\n",
    "#    model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "#    ### Running the model\n",
    "#\n",
    "#    \n",
    "#    return model\n",
    "#build_model_ = build_model()\n",
    "#build_model_.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=2000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_model_tune(hp):\n",
    "#    \n",
    "#    conv_size = hp.Int(\"conv_size\", min_value=16, max_value=64, step=8)\n",
    "#    kernel_size = hp.Int(\"kernel_size\", min_value=2, max_value=12, step=2)\n",
    "#    kernel_size_wide_n_tall = hp.Int(\"kernel_w_t_size\", min_value=2, max_value=3, step=1)\n",
    "#    dense_layer_size = hp.Int(\"dense_layer_size\", min_value=64, max_value=256, step=64)\n",
    "#    stride_val = hp.Int(\"stride_val\", min_value=64, max_value=256, step=64)\n",
    "#    maxpool_val =  hp.Int(\"maxpool_val\", min_value=2, max_value=10, step=1)\n",
    "#    \n",
    "#    model = Sequential()\n",
    "#    model.add(Conv3D(conv_size, kernel_size=(kernel_size, kernel_size_wide_n_tall, kernel_size_wide_n_tall),\n",
    "#                     input_shape=(60, 3, 3, 4), padding='same', data_format=\"channels_last\", strides = (1, 1, 1)))\n",
    "#    model.add(LeakyReLU(alpha=0.3))\n",
    "#    model.add(BatchNormalization())\n",
    "#    ##model.add(Conv3D(32, (3, 3, 5), activation=LeakyReLU(alpha=0.3), padding='same'))\n",
    "#    \n",
    "#    for i in range(hp.Int(\"n_layers\", 3, 7)):\n",
    "#        model.add(Conv3D(conv_size, (kernel_size, kernel_size_wide_n_tall, kernel_size_wide_n_tall), padding='same'))\n",
    "#        model.add(LeakyReLU(alpha=0.3))\n",
    "#        model.add(BatchNormalization())\n",
    "#    model.add(MaxPooling3D(pool_size=(maxpool_val, 3, 3)))\n",
    "#    model.add(Flatten())\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Dense(dense_layer_size))\n",
    "#    model.add(LeakyReLU(alpha=0.3))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Dense(dense_layer_size))\n",
    "#    model.add(LeakyReLU(alpha=0.3))\n",
    "#    model.add(Dropout(hp.Float('dropout_value', min_value = 0.0, max_value = 0.5, step=0.25)))\n",
    "#    model.add(Dense(1, activation='linear'))\n",
    "#\n",
    "#    ### Building the optimizer object\n",
    "#    #sgd = optimizers.SGD(lr = 0.1) # 1.0 0.0001 0.01\n",
    "#    opt = optimizers.Adam(lr = hp.Float('learning_rate', min_value = 0.0001, max_value = 0.1, step=0.0001)) \n",
    "#    ### Adding the loss function and optimizer to the model\n",
    "#    #mcp_save = ModelCheckpoint('CNN_3d_2x2_test_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "#    #opt = RAdam(total_steps=100, warmup_proportion=0.1, min_lr=1e-5)\n",
    "#    model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "#    ### Running the model\n",
    "#\n",
    "#    \n",
    "#    return model\n",
    "#LOG_DIR = \"test_\" + str(time.time())\n",
    "#tuner = RandomSearch(build_model_tune,\n",
    "#                    objective=\"val_mae\",\n",
    "#                     max_trials = 5,\n",
    "#                     executions_per_trial = 1,\n",
    "#                     directory = LOG_DIR)\n",
    "#tuner.search_space_summary()\n",
    "#tuner.search(x=X_train, y=Y_train,\n",
    "#             epochs=100,\n",
    "#             validation_split= 0.1,\n",
    "#             batch_size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "class HyperModel_555(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "    def build(self, hp):\n",
    "        conv_size = hp.Int(\"first_conv_size\", min_value=16, max_value=64, step=1)\n",
    "        kernel_size = hp.Int(\"kernel_size\", min_value=3, max_value=4, step=1)\n",
    "        dense_layer_size = hp.Int(\"dense_layer_size\", min_value=32, max_value=256, step=32)\n",
    "        max_pool_val = hp.Int(\"max_pool_val\", min_value=1, max_value=10, step=1)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(conv_size, kernel_size=(kernel_size, 2, 2),\n",
    "                         input_shape=self.input_shape, padding='same', data_format=\"channels_last\", strides = (1, 1, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(BatchNormalization())\n",
    "        ##model.add(Conv3D(32, (3, 3, 5), activation=LeakyReLU(alpha=0.3), padding='same'))\n",
    "\n",
    "        for i in range(hp.Int(\"n_layers\", 3, 7)):\n",
    "            model.add(Conv3D(hp.Int(f\"conv_{i}_size\", min_value=16, max_value=64, step=8), (kernel_size, 2, 2), padding='same'))\n",
    "            model.add(LeakyReLU(alpha=0.3))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(max_pool_val, 2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(dense_layer_size))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(dense_layer_size))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(hp.Float('dropout_value', min_value = 0.0, max_value = 0.5, step=0.1)))\n",
    "        \n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        ### Building the optimizer object\n",
    "        #sgd = optimizers.SGD(lr = 0.1) # 1.0 0.0001 0.01\n",
    "        opt = optimizers.Adam(lr = hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-7,\n",
    "                    max_value=0.05,\n",
    "                    default=0.00001,\n",
    "                    step=1e-7)) \n",
    "           \n",
    "        ### Adding the loss function and optimizer to the model\n",
    "        #mcp_save = ModelCheckpoint('CNN_3d_2x2_test_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "        #opt = RAdam(total_steps=100, warmup_proportion=0.1, min_lr=1e-5)\n",
    "        model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "        ### Running the model\n",
    "\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(run_directory)\n",
    "input_shape = (60, 2, 2, 4)\n",
    "hypermodel = HyperModel_555(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"bayes_rep_1_\"\n",
    "max_trials_val = 200\n",
    "# int(max_trials_val/10)\n",
    "tuner_bo = BayesianOptimization(\n",
    "            hypermodel,\n",
    "            objective='val_mae',\n",
    "            max_trials=max_trials_val,\n",
    "            seed=1,\n",
    "            executions_per_trial=2,\n",
    "            num_initial_points = int(max_trials_val/10),\n",
    "            directory = LOG_DIR)\n",
    "tuner_bo.search(x=X_train, y=Y_train,\n",
    "             epochs=100,\n",
    "             batch_size=int(len(X_train)/2),\n",
    "             validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_bo.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner_bo.get_best_models(num_models=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = keras.models.load_model(\"best_model_2_1_3ed_fns_5_mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=1000, batch_size=2500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = []\n",
    "for model in best_model:\n",
    "    pred_data.append(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, _ in enumerate(pred_data[0]):\n",
    "    print_str = ''\n",
    "    for count_model, __ in enumerate(pred_data):\n",
    "        #print(pred_data[count_model][0][0])\n",
    "        print_str += str(pred_data[count_model][count][0]) + \",\" \n",
    "    print(str(Y_train[count][0]) + \",\" + print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = open(\"out_2.csv\", \"w\")\n",
    "for count, _ in enumerate(pred_data[0]):\n",
    "    print_str = ''\n",
    "    for count_model, __ in enumerate(pred_data):\n",
    "        #print(pred_data[count_model][0][0])\n",
    "        print_str += str(pred_data[count_model][count][0]) + \",\" \n",
    "    outputfile.write(print_str+\"\\n\")\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
